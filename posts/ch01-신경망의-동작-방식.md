# [Ch_01] 신경망의 동작 방식

📅 2025-08-03T04:51:17.330Z

🔗 [원문 링크](https://velog.io/@son-dan-ha/Ch01-신경망의-동작-방식)

---




# Neural Networtk(NN)

 인공 신경망은 생물학적 신경망을 모방한다. 생물학에서는 뇌의 신경세포를 뉴런이라 하는데, 이 뉴런의 작용을 수학적으로 추상화하여 네트워크로 표현한 것이 **신경망**이다.
 
<br>

 
 # 1. 생물학적 뉴런
 
신경망이라는 단어에서 알 수 있듯 뉴런은 서로 **네트워크(망)**을 이룬다. 뉴런은 서로 **신호**를 받거나 보내며 **정보**를 처리한다. 

뉴런은 크게 축삭, 신경세포체, 수상돌기, 시냅스로 구성된다. 뉴런에 신호가 입력되면 신경세포체가 신호를 합하여 해당 합이 임곗값 보다 클 때에는 **다른 뉴런에 신호를 보내고** 작다면 보내지 않는다.

![](https://velog.velcdn.com/images/son-dan-ha/post/f33aa80c-abd6-4f72-9abc-b2b233b2a524/image.png)



<br>


---

# 2. 뉴런 활동의 수학적 표현

**뉴런의 활동**, 즉 **반응 구조**를 수학으로 표현할 수 있다. 

<br>

## 입력 신호

$\sum_{i=1}^{n} (x_i\cdot w_i)$

- 각각의 뉴런 : $x_i$, 가중치 : $w_i$

<br>

## 임계값 $\theta$

임계값 $\theta$에 따라 **출력 신호**가 있는 경우와 없는 경우로 구분된다.

$\begin{cases}\:\sum_{i=1}^{n} (x_i\cdot w_i) < \theta  \quad(y=0)\\\:\sum_{i=1}^{n} (x_i\cdot w_i) \geqq  \theta\quad(y=1)\end{cases}$



<br>

## 반응 조건 그래프

출력값을 0과 1로 본다면 **반응 조건 그래프**는 다음과 같은 모습을 갖는다. 

![](https://velog.velcdn.com/images/son-dan-ha/post/e57b24ab-06b8-4153-aa4d-a0f2fb54bbd1/image.png)


<br>


### 단위 계단 함수 (unit step function) 
해당 그래프를 **함수**로 일반화할 때, 이를 **단위 계단 함수** $u(z)$라고 한다. 

$u(z)=\begin{cases}\:0  \:(z<0)\\ \:1  \:(z\geqq0)\end{cases}$

<br>

### 반응 조건에 관한 식

$z=\sum_{i=1}^{n} (x_i\cdot w_i)-\theta$ 

$y=u(z)$

$y=u(\sum_{i=1}^{n} (x_i\cdot w_i)-\theta)$


<br><br>

---

# 3. 활성화 함수(Activation Function) 

해당 책에서는 뉴런을 생물학적 뉴런과 구별하기 위해 인공적 뉴런을 유닛(unit)이라 칭하므로 앞으로 유닛과 뉴런을 구분하여 사용한다.

앞서 뉴런의 출력 신호는 0과 1밖에 없었다. 유닛은 0, 1로 표현하지 않아도 된다. 

함수 $a$로 출력 신호를 수학적으로 일반화할 수 있다. 활성화 함수(전달 함수)를 사용자 정의 함수 $y=a(z)$로 설정할 수 있겠다.

<br>

## 뉴런과 유닛의 차이점


### 1. 활성화 함수 

- 뉴런 : 단위 계단 함수 $u(z)$
- 유닛 : 시그모이드 함수 $\sigma(z)$ 등..

>활성화 함수에는 많은 종류가 있다. 함수의 종류마다의 장단점이 있기에 현재는 ReLU 함수가 주로 사용된다. 
이 책에서는 계산의 간편함을 위해 활성화 함수로 시그모이드 함수를 사용한다.

<br>

### 2. 출력값 $y$

- 뉴런 : 0,1
- 유닛 : 활성화 함수를 사용할 수 있는 임의의 수

<br>    

### 3. 출력 신호 해석

- 뉴런 : 반응 여부
- 유닛 : 흥분도, 반응도, 활성도(activation)


<br><br>

## 시그모이드 함수

단위 계단 함수와의 가장 큰 차이는 시그모이드 함수는 **미분 가능**하며 **0과 1사이의 임의의 수**로 표현 가능하다.

(시그모이드 그래프 삽입) 

1에 가까우면 유닛의 흥분도(반응도)가 높다고 하고(민감), 0에 가까우면 유닛의 흥분도가 낮다(둔감)고 본다.

<br><br>

## 편향(bias)

뉴런의 신호 출력 여부를 결정하는 임계값 $\theta$을 **치환한 값**을 편향(bias)으로 설정한다. 

$b = - \theta$

$z=\sum_{i=1}^{n} (x_i\cdot w_i)+b$

$y=a(\sum_{i=1}^{n} (x_i\cdot w_i)+b)$

<br><br>

---

# 4. 신경망이란?

신경망은 뉴런의 네트워크로 구성된다면, 유닛을 네트워크로 구성하면 인공 신경망을 만들 수 있다. 

신경망은 크게 3개의 **layer(층)**로 나눌 수 있다. **입력층**, **은닉층(중간층)**, **출력층** 이다. 입력층은 입력 정보를 가져오고, 은닉층은 연산 결과를 전달하며 정보를 처리한다. 출력층은 최종 결과를 출력한다.

<br>

해당 책에서는 신경망 중 **필기체 숫자를 식별**하는 신경망을 예로 설명하고 있다. 해당 신경망을 기준으로 신경망의 개념과 구조를 이해해보자. (해당 신경망은 입력층의 유닛 전부가 중간층의 유닛으로 신호를 보내는 **완전 연결 계층** 구조를 가진다.) 


> 필기체 숫자 식별 신경망
- 4x3 크기의 픽셀
- 64개 흑백 학습 데이터
- 숫자 0과 1을 식별

<br>

## 입력층(input layer)

총 12개의 픽셀이 12개의 입력층에 전달된다. 입력층의 유닛은 입력과 출력이 동일하다. 항등함수를 사용해 $a(z)=z$로 표현할 수 있다.
<br>

## 출력층(output layer)

숫자 0과 1을 식별하려면 0에 반응하는 유닛과 1에 반응하는 유닛이 필요하다. 따라서 출력층은 2개의 유닛으로 구성된다. 
<br>


## 은닉층(hidden layer)

은닉층에서는 입력된 이미지의 특징을 추출한다. 은닉층이 몇개의 층으로 구성되고 몇개의 유닛으로 구성되는지는 아래에서 알아보자. 

<br><br>

---

# 5. 신경망의 구조

해당 책에서는 **은닉층의 특징 추출**을 추상화하기 위해 악마 조직의 정보망으로 예시를 든다. 

우선 악마들은 입력층, 은닉층, 출력층으로 3개의 위계로 표현할 수 있다. 입력층의 악마는 **부하 악마**로 픽셀 수에 맞게 총 12명이 있다. 부하 악마는 0과 1로 입력 신호를 3명의 **숨은 악마**에게 전달한다. 이 숨은 악마는 은닉층으로 볼 수 있다. 

앞서 은닉층의 역할이 특징 추출이라 했다. 따라서 숨은 악마는 각각 자신이 <u>선호하는 패턴(특징)</u>을 가지고 있다. **출력층의 악마**도 각각 선호하는 악마가 있다. 각각의 유닛에서 전달받는 신호는 이러한 선호에 따라 강하게 혹은 약하게 전달된다. 또한 각각의 악마는 <u>개성(마음의 편향)</u>이 있어 선호하지 않은 신호에 대해서는 걸러내게 된다.

![](https://velog.velcdn.com/images/son-dan-ha/post/3fa4b8e6-758e-4839-957a-e6b3fe275a15/image.png)


<br>


## 가중치와 편향

악마는 선호에 따른 관계가 있고 이는 **유닛의 가중치**로 비유할 수 있다. A 악마는 부하 악마 4, 7을 선호하기에 A로 가는 4와 7의 **신호는 크**다. 이를 **가중치가 크다**고 말할 수 있다. 
<br>

1. 악마 사이의 관계 
	- 관계가 좋으면 가중치가 크다.
    - 관계가 나쁘면 가중치가 작다.

    
2. 악마의 개성
	- 선호하지 않는 부하의 신호는 편향으로 걸러낸다.

<br>

## 숨은 악마의 수

신경망의 구조와 동작 방식을 악마의 네트워크로 비유했다. 흑백의 3X4 픽셀을 기준으로 0과 1을 분류하는 작업을 기준으로 했다. 

이때 의문을 가질 수 있는 부분은 악마의 수에 대해서다. 픽셀의 개수에 맞춰 부하 악마의 수는 12개인 것은 받아들일 수 있다. 하지만 숨은 악마는 왜 3명인가? 이는 0과 1을 식별하는 패턴을 3가지로 두었기 때문이다. 

사실 이 패턴에 따른 예측에 정확성에 대해서는 답에 따른 결과를 확인해야 알 수 있다. 이 과정에 대해서는 다음 장에서 수치적인 계산으로 알아보겠다.

<br><br>

---

# 6. 신경망의 학습

신경망은 스스로 학습하는 알고리즘을 가졌다. 학습을 통해 파라미터를 결정할 수 있다. 신경망의 파라미터 결정 방법에는 크게 지도학습과 비지도 학습으로 나눌 수 있다. (참고 : [머신러닝 종류](https://velog.io/@son-dan-ha/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EC%B4%88-%EC%A2%85%EB%A5%98))

지도학습이란 학습데이터를 가지고 가중치와 편향을 결정한다. 지도학습에서 신경망은 다음과 같은 방법으로 학습니다. 예측값과 정답 사시의 오차를 계산하여 이 호차들의 합이 최소가 되는 가중치와 편향을 결정한다. 이는 모델의 최적화라고도 한다. 

오차의 총합을 결정하는 다양한 방식이 있다. 이 책에서는 **제곱오차**를 활용한다.

- 제곱오차 = 비용함수 = $C_r$

제곱오차의 합을 최소화하여 파라미터를 결정하는 방법을 **최소제곱법**이라고 한다.


<br><br>

---

# 마무리
 
신경망의 구조를 설명하기 위해 악마의 네트워크로 비유해보았다. 인공신경망의 레이어(입력-은닉-출력)와 가중치와 편향이 신경망의 전체적 구조에서 어떤 의미를 갖는지 추상적으로 이해하는데에 도움이 되었다. 

다음 장에서는 수치적 계산을 통해 신경망의 학습을 알아본다. 추상적 이해와 더불어 수학적 수식으로 그 과정을 살펴본다면 딥러닝에 대한 이해의 레이어가 한 층 더 쌓일 것이다.

<br>





































 

