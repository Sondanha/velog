# [Ch_01] 신경망의 동작 방식

📅 2025-08-03T04:51:17.330Z

🔗 [원문 링크](https://velog.io/@son-dan-ha/Ch01-신경망의-동작-방식)

---




# Neural Networtk(NN)

 인공 신경망은 생물학적 신경망을 모방한다. 생물학에서는 뇌의 신경세포를 뉴런이라 하는데, 이 뉴런의 작용을 수학적으로 추상화하여 네트워크로 표현한 것이 **신경망**이다.
 
 # 1. 생물학적 뉴런
 
신경망이라는 단어에서 알 수 있듯 뉴런은 서로 **네트워크(망)**을 이룬다. 뉴런은 서로 **신호**를 받거나 보내며 **정보**를 처리한다. 

뉴런은 크게 축삭, 신경세포체, 수상돌기, 시냅스로 구성된다. 뉴런에 신호가 입력되면 신경세포체가 신호를 합하여 해당 합이 임곗값 보다 클 때에는 **다른 뉴런에 신호를 보내고** 작다면 보내지 않는다.

(뉴런 구조와 반응 구조에 대한 그림 삽입)


<br>


---

# 2. 뉴런 활동의 수학적 표현

이런 **뉴런의 활동**, 즉 **반응 구조**를 수학으로 표현할 수 있다. 


## 입력 신호

$\sum_{i=1}^{n} (x_i\cdot w_i)$

- 각각의 뉴런 : $x_i$, 가중치 : $w_i$

<br>

## 임계값 $\theta$

임계값 $\theta$에 따라 **출력 신호**가 있는 경우와 없는 경우로 구분된다.

$$\begin{cases}\sum_{i=1}^{n}(x_i\cdot w_i)<\theta\quad(y=0)\\\\sum_{i=1}^{n}(x_i\cdot w_i)\geqq\theta\quad(y=1)\end{cases}$$



<br>

## 반응 조건 그래프

출력값을 0과 1로 본다면 **반응 조건 그래프**는 다음과 같은 모습을 갖는다. 

(단위 계단 함수 그래프 삽입)



### 단위 계단 함수 (unit step function) 
해당 그래프를 **함수**로 일반화할 때, 이를 **단위 계단 함수** $u(z)$라고 한다. 

$$u(z)=\begin{cases}\:0  \:(z<0)\\ \:1  \:(z\geqq0)\end{cases}$$

### 반응 조건에 관한 식

$z=\sum_{i=1}^{n} (x_i\cdot w_i)-\theta$ 

$y=u(z)$

$y=u(\sum_{i=1}^{n} (x_i\cdot w_i)-\theta)$


<br>

---

# 3. 활성화 함수(Activation Function) 

해당 책에서는 뉴런을 생물학적 뉴런과 구별하기 위해 인공적 뉴런을 유닛(unit)이라 칭하므로 앞으로 유닛과 뉴런을 구분하여 사용한다.

앞서 뉴런의 출력 신호는 0과 1밖에 없었다. 유닛은 0, 1로 표현하지 않아도 된다. 

함수 $a$로 출력 신호를 수학적으로 일반화할 수 있다. 활성화 함수(전달 함수)를 사용자 정의 함수 $y=a(z)$로 설정할 수 있겠다.

<br>

## 뉴런과 유닛의 차이점


### 1. 활성화 함수 

- 뉴런 : 단위 계단 함수 $u(z)$
- 유닛 : 시그모이드 함수 $\sigma(z)$ 등..

>활성화 함수에는 많은 종류가 있다. 함수의 종류마다의 장단점이 있기에 현재는 ReLU 함수가 주로 사용된다. 
이 책에서는 계산의 간편함을 위해 활성화 함수로 시그모이드 함수를 사용한다.


### 2. 출력값 $y$

- 뉴런 : 0,1
- 유닛 : 활성화 함수를 사용할 수 있는 임의의 수

<br>    

### 3. 출력 신호 해석

- 뉴런 : 반응 여부
- 유닛 : 흥분도, 반응도, 활성도(activation)


<br>

## 시그모이드 함수

단위 계단 함수와의 가장 큰 차이는 시그모이드 함수는 **미분 가능**하며 **0과 1사이의 임의의 수**로 표현 가능하다.

(시그모이드 그래프 삽입) 

1에 가까우면 유닛의 흥분도(반응도)가 높다고 하고(민감), 0에 가까우면 유닛의 흥분도가 낮다(둔감)고 본다.

<br>

## 편향(bias)

뉴런의 신호 출력 여부를 결정하는 임계값 $\theta$을 **치환한 값**을 편향(bias)으로 설정한다. 

$b = - \theta$

$z=\sum_{i=1}^{n} (x_i\cdot w_i)+b$

$y=a(\sum_{i=1}^{n} (x_i\cdot w_i)+b)$

<br>

---

# 4. 신경망이란?

신경망은 뉴런의 네트워크로 구성된다면, 유닛을 네트워크로 구성하면 인공 신경망을 만들 수 있다. 

신경망은 크게 3개의 **layer(층)**로 나눌 수 있다. **입력층**, **은닉층(중간층)**, **출력층** 이다. 입력층은 입력 정보를 가져오고, 은닉층은 연산 결과를 전달하며 정보를 처리한다. 출력층은 최종 결과를 출력한다.

<br>

해당 책에서는 신경망 중 **필기체 숫자를 식별**하는 신경망을 예로 설명하고 있다. 해당 신경망을 기준으로 신경망의 개념과 구조를 이해해보자. (해당 신경망은 입력층의 유닛 전부가 중간층의 유닛으로 신호를 보내는 **완전 연결 계층** 구조를 가진다.) 


> 필기체 숫자 식별 신경망
- 4x3 크기의 픽셀
- 64개 흑백 학습 데이터
- 숫자 0과 1을 식별

<br>

## 입력층(input layer)

총 12개의 픽셀이 12개의 입력층에 전달된다. 입력층의 유닛은 입력과 출력이 동일하다. 항등함수를 사용해 $a(z)=z$로 표현할 수 있다.

## 출력층(output layer)

숫자 0과 1을 식별하려면 0에 반응하는 유닛과 1에 반응하는 유닛이 필요하다. 따라서 출력층은 2개의 유닛으로 구성된다. 


## 은닉층(hidden layer)

은닉층에서는 입력된 이미지의 특징을 추출한다. 은닉층이 몇개의 층으로 구성되고 몇개의 유닛으로 구성되는지는 아래에서 알아보자. 

<br>

---

# 5. 신경망의 구조

해당 책에서는 **은닉층의 특징 추출**을 추상화하기 위해 악마 조직의 정보망으로 예시를 든다. 




<br>

---


# 6. 신경망의 가중치와 편향





<br>

---

# 7. 신경망의 학습

<br>
<br>






































 

