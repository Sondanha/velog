# [웨비나 후기] npm 공급망 공격과 AI 모델 보안, 서비스 운영 관점에서 정리

📅 2026-01-21T15:19:30.999Z

🔗 [원문 링크](https://velog.io/@son-dan-ha/웨비나-후기-npm-공급망-공격과-AI-모델-보안-서비스-운영-관점에서-정리)

---

# 들어가며


<br>

오늘 [그로밋업 웨비나](https://event-us.kr/grometric/event/117958)를 들었다. 이번 웨비나는 **Sonatype Nexus Repository를 중심으로**, npm 오픈소스 공급망 공격 사례와 함께 **Hugging Face 기반 오픈소스 AI 모델의 보안 취약점**을 다루는 내용이었다.

웨비나를 듣게 된 계기는 단순하다. 작년 12월부터 본격적으로 시작한 프로젝트 CRATE에서 백엔드를 담당하고 있다. 현재는 AWS 배포 단계지만, MVP 이후에는 **AI 기능을 포함한 서비스 확장**을 계획하고 있고, 그 시점부터는 “기능 구현만 잘하면 된다”는 태도로는 운영이 불가능하다고 느꼈기 때문이다.

보안 쪽은 솔직히 아는 게 거의 없다. 다만 요즘은 개발자의 역할이 프론트엔드나 백엔드에만 국한되지 않고, **배포·운영·인프라·보안까지 전반적인 맥락을 이해해야 한다**는 이야기를 자주 듣는다. 특히 주니어에게도 이런 관점이 요구된다는 점에서, 지금부터라도 감을 잡아두는 게 필요하다고 생각했다.

<br><br>

---

# 오픈소스 공급망의 취약성

## 1. npm 중심의 오픈소스 생태계

현대 웹 서비스는 오픈소스 의존성 위에서 돌아간다. npm 생태계는 편리하지만, 그만큼 **공급망 공격의 표적**이 되기 쉽다.

* `npm install` 한 번
* 그 순간, 외부 코드가 내부 파이프라인으로 바로 유입됨
* 검증이 없다면 악성 코드도 같이 들어온다

즉, 개발 편의성과 보안 리스크가 동시에 커진 구조다.

<br>

## 2. 위협의 두 가지 유형

### 2-1. 악성코드(Malware) 위협

문제의 핵심은 **패키지 유입 단계에서의 검증 부재**다. 패키지에 대한 신뢰가 “작성자” 기준으로만 형성되어 있고, 개발자는 별 의심 없이 install 한다. 이때 CI/CD, 내부 네트워크까지 전파 가능하다.

<br>

#### **사례 1. debug / chalk 패키지 하이재킹**

* 사회공학 기법을 이용한 MFA 위장
* 정상 패키지 유지보수자의 계정 탈취
* 업데이트를 통해 악성 코드 유입

<br>

#### **사례 2. Shai-Hulud 2.0 캠페인**

* 단순 해킹이 아니라 **APT(지능형 지속 공격)**
* `bun` 런타임을 설치해 실행
* 자가 지속 사이클을 가진 **APT + Worm 구조**

<br>

Worm의 동작 방식은 꽤 충격적이었다.

* 저장소 순회
* 새로운 레포 생성
* GitHub Actions 워크플로 등록
* 자동 확산

개발자의 실수 한 번이 조직 전체로 번지는 구조다.

<br><br>

### 2-2. 내부 취약점(Vulnerability)

외부 공격이 아니어도 문제는 터진다.


#### **사례. React2Shell**

* 사용자 계정 정보나 엔드포인트 노출 없이 공격 가능
* CVSS 10.0 (최고 위험도)

> <공격 시나리오>
역직렬화 취약점 악용 
→ attacker가 victim 환경 접근 
→ `.env` 확인 
→ 파일 생성·삭제 가능

랜섬웨어였다면 서비스는 끝이다.


<br><br>


## 왜 수동 검증만으로는 부족한가

이쯤 되면 이런 생각이 든다.

> “리뷰 잘하고, 조심하면 되는 거 아닌가?”

현실은 다르다.

* 의존성 트리는 너무 깊다
* 모든 패키지 코드를 사람이 검토하는 건 불가능
* 사고 이후 패치는 이미 늦다

**Reactive(사후 대응)**가 아니라 **Proactive(반입 전 차단)**로 전략을 바꿔야 한다.


<br>


## Nexus가 제안하는 자동화 방어 전략

웨비나에서는 Nexus 기반의 **중앙 통제** 접근을 소개했다.

* 모든 개발·배포 트래픽 중앙화
* 문제 패키지 즉시 차단
* 검증된 버전만 캐싱
* 개발자는 “안전한 npm”만 보게 됨

개발 경험은 유지하면서, 리스크만 줄이는 방식이다.

<br><br>

---

# AI 시대의 새로운 공급망: Hugging Face

Hugging Face는 말 그대로 **AI의 GitHub**다.

* Models: 초대형 바이너리
* Datasets
* Spaces: ML 앱 호스팅 환경

문제는, 이 모든 것이 **신뢰 기반 공유**라는 점이다.

<br>

## OWASP LLM Top 10

OWASP는 LLM 기반 서비스를 운영할 때 발생할 수 있는 주요 보안 위협을 Top 10 형태로 정리했다. 전통적인 웹 보안이 “입력 → 코드 실행” 문제였다면, LLM 보안은 입력 → 추론 → 행동 문제에 가깝다.

#### 1. Prompt Injection
- 사용자 입력을 통해 시스템 프롬프트나 정책을 우회·조작하는 공격.
- LLM이 지켜야 할 규칙 자체가 무너진다.

#### 2. Insecure Output Handling

- LLM의 출력을 검증 없이 코드 실행, 렌더링, API 입력으로 사용하는 문제.
- XSS, 명령 실행, 권한 상승으로 이어질 수 있다.

#### 3. Training Data Poisoning

- 학습 데이터에 악성 또는 편향된 데이터를 주입해 모델의 행동을 왜곡하는 공격.
- 모델은 정상처럼 보이지만 결과는 신뢰할 수 없게 된다.

#### 4. Model Denial of Service (DoS)

- 과도하게 긴 입력, 반복 호출 등으로 추론 자원을 고갈시키는 공격.
- 서비스 장애와 비용 폭증을 동시에 유발한다.

#### 5. Supply Chain Vulnerabilities

- 모델, 라이브러리, 파이프라인 자체가 공격 지점이 되는 문제.
- npm 공급망 공격과 동일한 구조를 AI 영역에서 그대로 가진다.

#### 6. Sensitive Information Disclosure

- 프롬프트, 응답, 로그를 통해 개인정보, 인증 정보, 내부 데이터가 유출되는 문제.
- 의도하지 않은 데이터 회상이 주요 원인이다.

#### 7. Insecure Plugin / Tool Design

- LLM이 사용하는 플러그인·툴의 권한이 과도하게 설계된 경우.
- 모델이 공격자의 의도대로 시스템 자원을 조작할 수 있다.

#### 8. Excessive Agency

- LLM에 지나치게 많은 자율성과 실행 권한을 부여한 경우.
- 검증 없는 API 호출, 파일 조작, 외부 요청이 발생할 수 있다.

#### 9. Overreliance on LLM

- LLM의 출력을 검증 없이 신뢰하는 문제.
- 그럴듯한 오류가 시스템 의사결정으로 이어진다.

#### 10. Model Theft

- API 호출, 프롬프트 조합 등을 통해 모델의 동작을 추론·복제하는 공격.
- 모델 자체가 자산인 경우 치명적이다.

<br><br>

## 세 가지 핵심 과제

### 1. 인프라 문제

* 대용량 모델 다운로드 병목
* CI 환경에서 반복 다운로드
* 속도와 안정성 모두 문제

<br>

### 2. 거버넌스 문제

* Open source 모델
* Fine-tuning
* 내부 데이터 결합

> 이 과정에서 **AI Mutation**이 발생한다. 원본 모델의 학습 데이터와 리스크를 알 수 없는 상태로 변형된다.

<br>

### 3. 악성 모델 반입

* Pickle 역직렬화 구조 자체의 결함
* 모델 파일 자체가 공격 벡터가 될 수 있음

> 코드가 아닌 것처럼 취급되던 모델 파일이, 실제로는 실행 가능한 공격 벡터가 될 수 있다.

<br><br>

## AI 보안의 해법: npm과 같은 접근

해결책은 의외로 단순하다.

* Hugging Face 접근을 프록시화
* 로컬 캐싱
* 검증된 모델만 내부 반입

AI/ML 개발은 **모델**을 가져오고, **튜닝**하고, **다시 학습**하는 반복 과정이다. 한 번의 검증으로 끝나지 않는다.

<br>

## AIBOM이라는 개념

여기서 등장한 개념이 **AIBOM**이다. AI 시스템 전용 SBOM이라고 보면 된다.

포함 요소:

* 알고리즘
* 의존성
* 인프라
* 데이터 소스
* 메타데이터

AI를 “코드”가 아니라 **시스템 전체**로 관리하자는 관점이다.


<br><br>

---

# 마무리하며

이번 웨비나를 통해 느낀 건, AI 보안은 “나중에 고려할 문제”가 아니라 **모델을 쓰기로 결정하는 순간부터 함께 따라오는 전제**라는 점이었다. 앞으로 Hugging Face에 모델을 올리며 AI에 대한 이해를 넓혀갈 계획이고, 동시에 현재 프로젝트에 **실제 모델 도입을 고려하는 입장**이기 때문에 성능이나 정확도뿐만 아니라 **보안과 운영 관점**도 함께 생각해야 한다고 느꼈다.

지금은 NestJS 기반으로 백엔드를 빠르게 만들어가며 이른바 ‘바이브 코딩’을 하고 있지만,  
AI 기능을 서비스에 포함시키는 순간부터는 모델의 출처, 학습 데이터, 실행 권한, 공급망 구조 같은 요소들이 곧바로 운영 리스크로 이어진다는 점을 의식하게 됐다.

이번 정리는 보안을 깊이 알기 위한 기록이라기보다는, **실제로 운영하는 AI 서비스를 만들기 위한 출발점**에 가깝다.

<br><br>

