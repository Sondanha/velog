# [Ch_02] 1. 신경망의 필수 함수

📅 2025-10-27T17:00:59.694Z

🔗 [원문 링크](https://velog.io/@son-dan-ha/Ch02-1.-신경망의-필수-함수)

---

두번째 장에서는 신경망을 위한 수학 기초를 복습한다. 우선 필수 함수를 가볍게 살펴보자.

# 1차 함수

1차 함수는 직선이다. 

![](https://velog.velcdn.com/images/son-dan-ha/post/bb0c2ac1-37b1-4605-aa29-77fc7a37815a/image.png)


## 1. 절편과 기울기

<u>좌표평면에서 그래프가 축과 만나는 지점</u>을 **절편**이라고 한다. $y=x+1$에서 x절편은 -1, y절편은 1이다. 

**기울기**는 <u>x값의 변화량에 대한 y값의 변화량</u>을 말한다. 1차 함수는 그 변화량이 어느 지점에서든 일정하다. $y=x+1$ 의 기울기는 1이다. ($\frac{dy}{dx}=1$)

딥러닝에서 **gradient**(경사)가 바로 기울기다. **경사 하강법**을 공부할 때 자세히 알아보도록 하자. 



<br>

## 2. 독립변수와 종속변수

$y=x+1$에서 변수는 $y, x$로 두 종류가 있다. $x$가 주어졌을 때 $y$가 결정되는 관계라면 x를 **독립변수**, y를 **종속변수**라고 한다. 

### 초평면(hyperplane)
독립변수는 $y=ax_1+bx_2+c$처럼 여러 개일 수 있다. ($a\neq0, b\neq0$) 딥러닝에서는 x를 특징(feature)으로 본다. 보통 하나 이상의 특징이 있으므로 여러 개의 독립변수가 있는 상황이 일반적이다. 

앞서 1차 함수를 직선이라고 했지만, 독립변수가 n개인 1차 함수는 기하학적 공간에서 ‘초평면(hyperplane)’으로 표현된다.
예를 들어 독립변수가 1개일 때는 2차원 공간의 직선, 독립변수가 2개일 때는 3차원 공간의 평면이 된다. 이것을 일반화하면 n개의 독립변수를 가진 함수는 (n+1)차원 공간에서 초평면이 된다.

아래는 3차원의 공간에서 평면으로 표현된 $z=x+y$ 그래프다.

![](https://velog.velcdn.com/images/son-dan-ha/post/230688d2-191d-4f87-b218-41f59c793ff1/image.png)

초평면은 **분류(Classification)**에서 **결정경계(boundary)**가 된다는 것을 알고 있으면 된다. 


<br>

## 3. 가중입력 $z$

앞서 신경망에서는 뉴닛이 받는 가중 입력 $z$가 1차 함수 관계 $z=w_1x_1+w_2x_2+w_3x_3+b$ 로 표현되었다. 

가중치 $w$와 편향 $b$를 상수 파라미터라고 보면 가중 입력 $z$는 $x_1, x_2,x_3$와 1차 함수 관계이다.



<br><br>

---

# 2차 함수

2차 함수는 포물선이다. 

![](https://velog.velcdn.com/images/son-dan-ha/post/1125ea1f-af5e-4ea9-b8ae-c16ea6e089a7/image.png)


## 1. 최솟값과 최댓값

2차 함수는 최솟값과 최댓값을 갖는다는 특징이 있다. $y=x^2-1$ 처럼 최대차항이 양수라면 최솟값이 존재하고 $y=-x^2+1$ 처럼 음수라면 최댓값이 존재한다.

### 최소제곱법

2차 함수의 최솟값은 딥러닝에서 최소 제곱법의 기본이 된다. 최소제곱법은 “**오차 제곱합**”이라는 2차 함수를 만들고,
그 함수의 최솟값을 찾는 과정을 말한다. 신경망에서는 **최적화** 과정을 생각하면 된다. 


<br>

## 2. hypersurface

2차 함수는 독립변수가 여러 개일 때 hypersurface가 된다. 1차 함수가 고차원 공간에서 초평면을 만든다면, 2차 함수는 그에 대응되는 곡면(포물면)을 만든다.

아래는 3차원 공간에서 곡면으로 표현된 $z=x^2+y^2-2$ 그래프다.

![](https://velog.velcdn.com/images/son-dan-ha/post/d04da4d0-dcb3-4882-8fa8-3a2d33b8d6e0/image.png)


<br><br>

---

# 지수함수

지수함수는 $y=a^x$ ($a$는 1이 아닌 양의 상수) 형태를 갖는다. 이때 $a$를 **밑(base)**이라고 하며 딥러닝에서는 $a$ 값으로 주로 자연상수 $e$가 들어간다. 

![](https://velog.velcdn.com/images/son-dan-ha/post/0f671e80-b24c-40d4-a3d3-686623da5a45/image.png)



## 1. 자연상수 $e$

자연상수 $e$는 딥러닝의 지수적 표현에서 매우 자주 등장하는 상수이다. 이 값은 단순한 숫자가 아니라, **특정 수열의 극한**으로 정의된다. 아래에서는 이 수열이 **단조수렴**함을 보이고, 그 극한값이 $e$가 됨을 살펴본다.

$a_n=(1+\frac{1}{n})^n$라는 수열을 생각하자. 

이 수열이 수렴한다면, 그 극한값을 자연상수 $e$라고 정의할 수 있다. 수렴은 단조와 유계, 두 가지를 확인한다. 단조는 쭉 증가하거나 감소하는 형태를 말하고 유계는 상하한이 존재하는 경우를 말한다. 

### 수열 $a_n$이 단조 증가 증명

#### 1. 보조부등식 
$\ln(1+x)\ge\frac{x}{1+x}$ ($x>-1$) 
![](https://velog.velcdn.com/images/son-dan-ha/post/3a1b172c-c104-499c-b65d-c2b281adf557/image.png)

$\ln x$는 오목(concave)함수다. 오목 함수는 항상 접선보다 아래에 있으며, 평균기울기 ≥ 순간기울기 라는 특징을 가진다. 

$\frac{f(x)-f(0)}{x}\ge f'(x)$ 에서 $f(x)=\ln(1+x)$ 라면,

$\frac{\ln(1+x)-\ln 1}{x}\ge \frac{1}{1+x}$

$\frac{\ln(1+x)}{x}\ge \frac{1}{1+x}$

$\ln(1+x)\ge \frac{x}{1+x}$ 으로 해당 보조부등식을 활용할 수 있다.

<br>

#### 2. 단조 증가 증명

$a_n = \left(1+\frac{1}{n}\right)^n$에 양변 로그, $\ln a_n = n \ln\left(1+\frac{1}{n}\right)$

1의 보조부등식에서 $x=\frac{1}{m}$ 라면,
$\ln\left(1+\frac{1}{m}\right) \ge \frac{1}{m+1}$


이제 $\ln a_{n+1} - \ln a_n> 0$ 를 증명하면 된다.

$=(n+1)\ln\left(1+\frac{1}{n+1}\right) - n\ln\left(1+\frac{1}{n}\right)$

$\ge \frac{n+1}{n+2} - \frac{n}{n+1} = \frac{1}{(n+1)(n+2)} > 0$





출처: https://youtu.be/z1xeFi5N5go?si=exA38Z-Sy4ZATQtW


<br>

## 2. 시그모이드 함수





<br><br>

---

# 확률밀도 함수



























